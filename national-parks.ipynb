{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Historical Visitation Numbers of U.S. National Parks\n",
    "https://github.com/carl-schick-ds/national-parks\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project will analyze the historical visitation counts for each U.S. National Park and attempt to answer the following questions: \n",
    " - How does the age of a national park impact annual visitation?\n",
    " - How does the area-size of a national park impact annual visitation?\n",
    " - Are visitation numbers impacted when a site receives an official national park designation?\n",
    " - Does the location of a national park impact annual visitation?\n",
    " - How does the weather impact monthly visitation?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "The first step of the project is to collect the data from various sources on the internet.  \n",
    "\n",
    "Data is currently collected from two main sources...\n",
    " - The National Park Services' Integrated Resourcde Management Applications (IRMA) Portal\n",
    "   - List of official national park unit codes\n",
    "   - Monthly visitation counts for each national park\n",
    " - Wikipedia\n",
    "   - Location (lat/long), date established, and gross area acres for each national park\n",
    "\n",
    "Web scraping is performed using the [Beautfiul Soup](https://www.crummy.com/software/BeautifulSoup/) library.  \n",
    "Fuzzy matching logic for park names is performed using the [The Fuzz (formerly Fuzzy Wuzzy)](https://github.com/seatgeek/thefuzz) library.\n",
    "\n",
    "**_Data collection will rebuild/refresh the CSV files used for the remainder of the notebook._**  \n",
    "**_Data collection can be toggled on/off with the REFRESH_DATA boolean flag._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFRESH_DATA = False\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if REFRESH_DATA:\n",
    "    # pip install thefuzz\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    from thefuzz import fuzz\n",
    "    from thefuzz import process\n",
    "    import requests\n",
    "    import time\n",
    "    import re\n",
    "    user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'\n",
    "    headers = {'User-Agent': user_agent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REFRESH_DATA:\n",
    "    from requests.adapters import HTTPAdapter\n",
    "    from requests.packages.urllib3.util.retry import Retry\n",
    "    retry_strategy = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    http = requests.Session()\n",
    "    http.mount(\"https://\", adapter)\n",
    "    http.mount(\"http://\", adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REFRESH_DATA:\n",
    "\n",
    "    park_units_url = 'https://irmaservices.nps.gov/v2/rest/unit/designations'\n",
    "    park_units_namespace = {'root': 'NRPC.IrmaServices.Rest.Unit'}\n",
    "    park_unit_exceptions = {'DENG':'DENA', 'GAAG':'GAAR', 'GLBG':'GLBA', 'GRDG':'GRSA', 'KATG':'KATM', 'LACG':'LACL', 'WRSG':'WRST'}\n",
    "    \n",
    "    national_parks = {}\n",
    "    r = http.get(park_units_url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, \"xml\")\n",
    "    # print(soup)\n",
    "\n",
    "    for unit_designation in soup.find_all('UnitDesignation'):\n",
    "        if unit_designation.find(\"Code\").text == 'NP':\n",
    "            units = unit_designation.find(\"Units\")\n",
    "            for value in units.find_all('Value'):\n",
    "                raw_code = value.find(\"Code\").text\n",
    "                name = value.find(\"Name\").text\n",
    "                code = raw_code if raw_code not in park_unit_exceptions.keys() else park_unit_exceptions[raw_code]\n",
    "                national_parks[code] = name\n",
    "\n",
    "    # Manually add New River Gorge, if it hasn't been added yet\n",
    "    if 'NERI' not in national_parks.keys():\n",
    "        national_parks['NERI'] = 'New River Gorge'\n",
    "\n",
    "    # print(national_parks)\n",
    "    park_units_df = pd.DataFrame.from_dict(national_parks, orient='index', columns=['name'])\n",
    "    park_units_df.index.name = 'code'\n",
    "    park_units_df.sort_index(inplace=True)\n",
    "    park_units_df.to_csv('national_park_units.csv')\n",
    "    # print(park_units_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REFRESH_DATA:\n",
    "\n",
    "    # park_visits_homepage = 'https://irma.nps.gov/STATS/'\n",
    "    park_visits_domain = 'https://irma.nps.gov'\n",
    "    park_visits_url = '/STATS/SSRSReports/Park%20Specific%20Reports/Recreation%20Visitors%20By%20Month%20(1979%20-%20Last%20Calendar%20Year)'\n",
    "    park_visits_qs = '?Park='\n",
    "    park_visits_df = pd.DataFrame()\n",
    "    target_table_min = 10\n",
    "\n",
    "    print('Processing:', end=\" \")\n",
    "    for park_code in park_units_df.index:\n",
    "        print(park_code, end=\", \")\n",
    "        park_visits_request = park_visits_domain + park_visits_url + park_visits_qs + park_code\n",
    "        r = http.get(park_visits_request, headers=headers, timeout=5)\n",
    "        soup = BeautifulSoup(r.text, \"html\")\n",
    "        park_visits_iframe = soup.find('iframe').attrs['src']\n",
    "\n",
    "        park_visits_request = park_visits_domain + park_visits_iframe\n",
    "        r = http.get(park_visits_request, headers=headers, timeout=5)\n",
    "\n",
    "        dfs = pd.read_html(r.text, match=\"Year\", skiprows=1)\n",
    "        for df in dfs:\n",
    "            if len(df) > target_table_min: one_park_df = df\n",
    "        \n",
    "        new_header = one_park_df.iloc[0] #grab the first row for the header\n",
    "        one_park_df = one_park_df[1:] #take the data less the header row\n",
    "        one_park_df.columns = new_header #set the header row as the df header\n",
    "    \n",
    "        one_park_df = one_park_df.fillna(0)\n",
    "        if 'Total' in one_park_df.columns:\n",
    "            one_park_df.drop('Total', axis=1, inplace=True)\n",
    "\n",
    "        one_park_df.set_index('Year', inplace=True)\n",
    "        one_park_srs = one_park_df.stack()\n",
    "\n",
    "        park_visits_df[park_code] = one_park_srs\n",
    "\n",
    "    park_visits_df = park_visits_df.fillna(0)\n",
    "    park_visits_df.index.names = ['Year', 'Month']\n",
    "    park_visits_df.to_csv('national_park_visits.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REFRESH_DATA:\n",
    "\n",
    "    park_data_url = 'https://en.wikipedia.org/wiki/List_of_national_parks_of_the_United_States'\n",
    "    \n",
    "    r = http.get(park_data_url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, \"xml\")\n",
    "    # print(soup)\n",
    "\n",
    "    dfs = pd.read_html(r.text, match=\"Date established as park\")\n",
    "    park_data_df = dfs[0]\n",
    "\n",
    "    # Strip years and footnotes from columns\n",
    "    for column_name in park_data_df.columns.values:\n",
    "        if '(' in column_name or '[' in column_name:\n",
    "            new_column_name = re.sub(\"\\(.*?\\)\",\"\", column_name)\n",
    "            new_column_name = re.sub(\"\\[.*?\\]\",\"\", new_column_name)\n",
    "            new_column_name = new_column_name.strip()\n",
    "            park_data_df.rename(columns={column_name: new_column_name}, inplace=True)\n",
    " \n",
    "    # Rename some columns and drop unneeded columns\n",
    "    park_data_df.rename(columns={'Date established as park': 'Established', 'Area': 'Acres'}, inplace=True)\n",
    "    park_data_df.drop(columns=['Image', 'Recreation visitors'], inplace=True)\n",
    "\n",
    "    # Remove asterisks from the park names\n",
    "    park_data_df['Name'] = park_data_df['Name'].str.replace('*', '').str.strip()\n",
    "\n",
    "    # Add each park's unit code\n",
    "    # We will need fuzzy string matching logic to match the name from Wikipedia with the name from NPS\n",
    "    nps_names = park_units_df['name'].to_list()\n",
    "    # print(nps_names)\n",
    "    for index, row in park_data_df.iterrows():\n",
    "        # print(park_data_df.loc[index,'Name'])\n",
    "        matching_name, matching_ratio = process.extractOne(park_data_df.loc[index,'Name'], nps_names)\n",
    "        # print(matching_name, matching_ratio)\n",
    "        # print(park_units_df[park_units_df['name'] == matching_name].index.tolist()[0])\n",
    "        park_data_df.loc[index,'Code'] = park_units_df[park_units_df['name'] == matching_name].index.tolist()[0]\n",
    "    \n",
    "    # Parse state from Location into new column; update Location to only lat/long coordinates\n",
    "    park_data_df['State'] = park_data_df['Location'].apply(lambda x: re.split('[^a-zA-Z\\s\\.]', x)[0].replace('.mw', ''))\n",
    "    park_data_df['Location'] = park_data_df['Location'].apply(lambda x: x.rpartition('/')[2].strip())\n",
    "\n",
    "    # Remove footnotes from established dates\n",
    "    park_data_df['Established'] = park_data_df['Established'].apply(lambda x: re.sub('\\[\\d*?\\]', '', x).strip())\n",
    "\n",
    "    # Clean Acres column\n",
    "    park_data_df['Acres'] = park_data_df['Acres'].apply(lambda x: re.split('[^\\d\\,\\.]', x)[0])\n",
    "\n",
    "    park_data_df = park_data_df[['Code', 'Name', 'State', 'Location', 'Established', 'Acres', 'Description']]\n",
    "\n",
    "    park_data_df.set_index('Code', inplace=True)\n",
    "    park_data_df.to_csv('national_park_data.csv')\n",
    "    # print(park_data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'national_park_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kt/b8lrpwpn5w9910rcrffl5p140000gp/T/ipykernel_7273/2379332259.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpark_visits_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'national_park_visits.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpark_data_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'national_park_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpark_visits_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpark_data_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/np/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/np/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/np/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/np/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/np/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/np/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/np/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/np/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'national_park_data.csv'"
     ]
    }
   ],
   "source": [
    "park_visits_df = pd.read_csv('national_park_visits.csv', index_col=[0,1])\n",
    "park_data_df = pd.read_csv('national_park_data.csv', index_col=0)\n",
    "print(park_visits_df.head())\n",
    "print(park_data_df.head())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12e770f971261f02467e136ae451d4f5bb6defd0f5ed675545d9d042537fc757"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('np': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
