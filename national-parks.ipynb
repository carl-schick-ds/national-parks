{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Historical Visitation Numbers of U.S. National Parks\n",
    "https://github.com/carl-schick-ds/national-parks\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project will analyze the historical visitation counts for each U.S. National Park and attempt to answer the following questions: \n",
    " - How does the age of a national park impact annual visitation?\n",
    " - How does the area-size of a national park impact annual visitation?\n",
    " - Are visitation numbers impacted when a site receives an official national park designation?\n",
    " - Does the location of a national park impact annual visitation?\n",
    " - How does the weather impact monthly visitation?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "The first step of the project is to collect the data from various sources on the internet.  \n",
    "\n",
    "Data is currently collected from two main sources...\n",
    " - The National Park Services' Integrated Resourcde Management Applications (IRMA) Portal\n",
    "   - List of official national park unit codes\n",
    "   - Monthly visitation counts for each national park\n",
    " - Wikipedia\n",
    "   - Location (lat/long), date established, and gross area acres for each national park\n",
    "\n",
    "Web scraping is performed using the [Beautfiul Soup](https://www.crummy.com/software/BeautifulSoup/) library.\n",
    "\n",
    "**_Data collection will rebuild/refresh the CSV files used for the remainder of the notebook._**  \n",
    "**_Data collection can be toggled on/off with the REFRESH_DATA boolean flag._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFRESH_DATA = True\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if REFRESH_DATA:\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    import time\n",
    "    import re\n",
    "    user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'\n",
    "    headers = {'User-Agent': user_agent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REFRESH_DATA:\n",
    "    from requests.adapters import HTTPAdapter\n",
    "    from requests.packages.urllib3.util.retry import Retry\n",
    "    retry_strategy = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    http = requests.Session()\n",
    "    http.mount(\"https://\", adapter)\n",
    "    http.mount(\"http://\", adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REFRESH_DATA:\n",
    "\n",
    "    park_units_url = 'https://irmaservices.nps.gov/v2/rest/unit/designations'\n",
    "    park_units_namespace = {'root': 'NRPC.IrmaServices.Rest.Unit'}\n",
    "    park_unit_exceptions = {'DENG':'DENA', 'GAAG':'GAAR', 'GLBG':'GLBA', 'GRDG':'GRSA', 'KATG':'KATM', 'LACG':'LACL', 'WRSG':'WRST'}\n",
    "    \n",
    "    national_parks = {}\n",
    "    r = http.get(park_units_url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, \"xml\")\n",
    "    # print(soup)\n",
    "\n",
    "    for unit_designation in soup.find_all('UnitDesignation'):\n",
    "        if unit_designation.find(\"Code\").text == 'NP':\n",
    "            units = unit_designation.find(\"Units\")\n",
    "            for value in units.find_all('Value'):\n",
    "                raw_code = value.find(\"Code\").text\n",
    "                name = value.find(\"Name\").text\n",
    "                code = raw_code if raw_code not in park_unit_exceptions.keys() else park_unit_exceptions[raw_code]\n",
    "                national_parks[code] = name\n",
    "\n",
    "    # print(national_parks)\n",
    "    park_units_df = pd.DataFrame.from_dict(national_parks, orient='index', columns=['name'])\n",
    "    park_units_df.index.name = 'code'\n",
    "    park_units_df.to_csv('national_park_units.csv')\n",
    "    # print(park_units_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ACAD, ARCH, BADL, BIBE, BISC, BLCA, BRCA, CANY, CARE, CAVE, CHIS, CONG, CRLA, CUVA, DENA, DEVA, DRTO, EVER, GAAR, GLAC, GLBA, GRBA, GRCA, GRSA, GRSM, GRTE, GUMO, HALE, HAVO, HOSP, INDU, ISRO, JEFF, JOTR, KATM, KEFJ, KICA, KOVA, LACL, LAVO, MACA, MEVE, MORA, NOCA, NPSA, OLYM, PEFO, PINN, REDW, ROMO, SAGU, SEQU, SHEN, THRO, VIIS, VOYA, WHSA, WICA, WRST, YELL, YOSE, ZION, "
     ]
    }
   ],
   "source": [
    "if REFRESH_DATA:\n",
    "\n",
    "    # park_visits_homepage = 'https://irma.nps.gov/STATS/'\n",
    "    park_visits_domain = 'https://irma.nps.gov'\n",
    "    park_visits_url = '/STATS/SSRSReports/Park%20Specific%20Reports/Recreation%20Visitors%20By%20Month%20(1979%20-%20Last%20Calendar%20Year)'\n",
    "    park_visits_qs = '?Park='\n",
    "    park_visits_df = pd.DataFrame()\n",
    "    target_table_min = 10\n",
    "\n",
    "    print('Processing:', end=\" \")\n",
    "    for park_code in park_units_df.index:\n",
    "        print(park_code, end=\", \")\n",
    "        park_visits_request = park_visits_domain + park_visits_url + park_visits_qs + park_code\n",
    "        r = http.get(park_visits_request, headers=headers, timeout=5)\n",
    "        soup = BeautifulSoup(r.text, \"html\")\n",
    "        park_visits_iframe = soup.find('iframe').attrs['src']\n",
    "\n",
    "        park_visits_request = park_visits_domain + park_visits_iframe\n",
    "        r = http.get(park_visits_request, headers=headers, timeout=5)\n",
    "\n",
    "        dfs = pd.read_html(r.text, match=\"Year\", skiprows=1)\n",
    "        for df in dfs:\n",
    "            if len(df) > target_table_min: one_park_df = df\n",
    "        \n",
    "        new_header = one_park_df.iloc[0] #grab the first row for the header\n",
    "        one_park_df = one_park_df[1:] #take the data less the header row\n",
    "        one_park_df.columns = new_header #set the header row as the df header\n",
    "    \n",
    "        one_park_df = one_park_df.fillna(0)\n",
    "        if 'Total' in one_park_df.columns:\n",
    "            one_park_df.drop('Total', axis=1, inplace=True)\n",
    "\n",
    "        one_park_df.set_index('Year', inplace=True)\n",
    "        one_park_srs = one_park_df.stack()\n",
    "\n",
    "        park_visits_df[park_code] = one_park_srs\n",
    "\n",
    "    park_visits_df = park_visits_df.fillna(0)\n",
    "    park_visits_df.index.names = ['Year', 'Month']\n",
    "    park_visits_df.to_csv('national_park_visits.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Name        Established Acres  \\\n",
      "0          Acadia  February 26, 1919         \n",
      "1  American Samoa   October 31, 1988         \n",
      "2          Arches  November 12, 1971         \n",
      "3        Badlands  November 10, 1978         \n",
      "4        Big Bend      June 12, 1944         \n",
      "\n",
      "                                         Description           State  \\\n",
      "0  Covering most of Mount Desert Island and other...           Maine   \n",
      "1  The southernmost national park is on three Sam...  American Samoa   \n",
      "2  This site features more than 2,000 natural san...            Utah   \n",
      "3  The Badlands are a collection of buttes, pinna...    South Dakota   \n",
      "4  Named for the prominent bend in the Rio Grande...           Texas   \n",
      "\n",
      "            Lat-Long  \n",
      "0   ﻿44.35°N 68.21°W  \n",
      "1  ﻿14.25°S 170.68°W  \n",
      "2  ﻿38.68°N 109.57°W  \n",
      "3  ﻿43.75°N 102.50°W  \n",
      "4  ﻿29.25°N 103.25°W  \n"
     ]
    }
   ],
   "source": [
    "if REFRESH_DATA:\n",
    "\n",
    "    park_data_url = 'https://en.wikipedia.org/wiki/List_of_national_parks_of_the_United_States'\n",
    "    \n",
    "    r = http.get(park_data_url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, \"xml\")\n",
    "    # print(soup)\n",
    "\n",
    "    dfs = pd.read_html(r.text, match=\"Date established as park\")\n",
    "    park_data_df = dfs[0]\n",
    "\n",
    "    # Strip years and footnotes from columns\n",
    "    for column_name in park_data_df.columns.values:\n",
    "        if '(' in column_name or '[' in column_name:\n",
    "            new_column_name = re.sub(\"\\(.*?\\)\",\"\", column_name)\n",
    "            new_column_name = re.sub(\"\\[.*?\\]\",\"\", new_column_name)\n",
    "            new_column_name = new_column_name.strip()\n",
    "            park_data_df.rename(columns={column_name: new_column_name}, inplace=True)\n",
    " \n",
    "    # Rename some columns and drop unneeded columns\n",
    "    park_data_df.rename(columns={'Date established as park': 'Established', 'Area': 'Acres'}, inplace=True)\n",
    "    park_data_df.drop(columns=['Image', 'Recreation visitors'], inplace=True)\n",
    "\n",
    "    # Parse state and lat/long from Location into new columns; then drop Location\n",
    "    park_data_df['State'] = park_data_df['Location'].apply(lambda x: re.split('[^a-zA-Z\\s]', x)[0])\n",
    "    park_data_df['Lat-Long'] = park_data_df['Location'].apply(lambda x: x.rpartition('/')[2].strip())\n",
    "    park_data_df.drop(columns=['Location'], inplace=True)\n",
    "\n",
    "    # Clean Acres column to float\n",
    "    park_data_df['Acres'] = park_data_df['Acres'].apply(lambda x: re.split('[0-9,.]', x)[0])\n",
    "\n",
    "    print(park_data_df.head())\n",
    "\n",
    "    # print(national_parks)\n",
    "    # national_parks_df = pd.DataFrame.from_dict(national_parks, orient='index', columns=['name'])\n",
    "    # national_parks_df.index.name = 'code'\n",
    "    # national_parks_df.to_csv('national_parks.csv')\n",
    "    # print(national_parks_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ACAD    ARCH    BADL   BIBE   BISC   BLCA    BRCA    CANY  \\\n",
      "Year Month                                                                \n",
      "2021 JAN     20268   48725   14899  50298  25626   8759   37725   18837   \n",
      "     FEB     16123   53986   11350  43889  19596   7287   35653   20164   \n",
      "     MAR     38060  151074   28182  89915  30655   7236   96800   87001   \n",
      "     APR    110096  193912   42749  67628  30835  14159  201771  116971   \n",
      "     MAY    324654  225789  109764  53067  37128  34729  297905  134778   \n",
      "\n",
      "              CARE   CAVE  ...    SHEN   THRO   VIIS   VOYA   WHSA   WICA  \\\n",
      "Year Month                 ...                                              \n",
      "2021 JAN     20786  16219  ...   37208   6866  18736   4679  45053  14840   \n",
      "     FEB     25718  13270  ...    9771   5162  19680   7887  35766  10836   \n",
      "     MAR    115728  30749  ...   75294  15125  22302   1558  98118  16430   \n",
      "     APR    165498  25859  ...  114583  11993  26531    104  78723  43241   \n",
      "     MAY    221206  29206  ...  148399  76738  26367  29785  85197  77241   \n",
      "\n",
      "            WRST    YELL    YOSE    ZION  \n",
      "Year Month                                \n",
      "2021 JAN      25   35338   67284  172747  \n",
      "     FEB      95   36897  128222  160232  \n",
      "     MAR     230   35611  159906  429455  \n",
      "     APR     520   67508  257158  469775  \n",
      "     MAY    2003  473799  347005  610301  \n",
      "\n",
      "[5 rows x 62 columns]\n"
     ]
    }
   ],
   "source": [
    "park_visits_df = pd.read_csv('national_park_visits.csv', index_col=[0,1])\n",
    "print(park_visits_df.head())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12e770f971261f02467e136ae451d4f5bb6defd0f5ed675545d9d042537fc757"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('np': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
